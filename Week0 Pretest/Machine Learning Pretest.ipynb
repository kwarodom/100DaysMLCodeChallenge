{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pretest\n",
    "## First Part: Multiple Choices\n",
    "## https://goo.gl/forms/emElBoR6SeBAAvMN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part: Coding Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "### Simulate N coin flips and count number of head and number of tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def simulate_coin_flips(N, p):\n",
    "    \"\"\"\n",
    "    return (number of head, number of tail)\n",
    "    \"\"\"\n",
    "    result = {'H':0, 'T':0}\n",
    "    for _ in range(N):\n",
    "        _ran = random.choices(['H','T'], weights=(p, 1-p) )\n",
    "        result[_ran[0]]+=1\n",
    "    return result['H'],result['T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate_coin_flips(10, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### Write a program to simulate data and answer this question: How many coin flips on average does it take to get n consecutive heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_coin_flips_until_n_heads(n, p):\n",
    "    \"\"\"\n",
    "    return the answer\n",
    "    \"\"\"\n",
    "    consecutive_count = 0\n",
    "    toss_count = 0\n",
    "        \n",
    "    while consecutive_count < n:\n",
    "        _ran = random.choices(['H','T'], weights=(p, 1-p) )\n",
    "        if _ran[0] == 'H':\n",
    "            consecutive_count +=1\n",
    "        toss_count+=1\n",
    "        \n",
    "             \n",
    "    return toss_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 1391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_coin_flips_until_n_heads(100, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "### Fit linear regression on the simulated data below and show that the fitted parameters are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.normal(size=(100,2))\n",
    "y = 4 + 3 * x[:,0] + 5 * x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.060899114889893e-31"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([3,5])\n",
    "b = 4\n",
    "np.mean(np.square(y - (x.dot(w)+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.1):\n",
    "        self.weigths = None\n",
    "        self.bias = 0\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        # init weigths parameter\n",
    "        self.weigths = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        # learn until cost not improve\n",
    "        while(True):\n",
    "            pred_a = self.predict(X)\n",
    "            cost_a = self.cost_fn(Y,pred_a)\n",
    "            \n",
    "            grad_w, grad_b = self.gradient(X,Y,pred_a)\n",
    "            \n",
    "            # update weigth parameters\n",
    "            self.weigths = self.weigths - self.learning_rate*grad_w\n",
    "\n",
    "            # update bias parameter\n",
    "            self.bias = self.bias - self.learning_rate*grad_b\n",
    "            \n",
    "            pred_b = self.predict(X)\n",
    "            cost_b = self.cost_fn(Y,pred_b)\n",
    "            \n",
    "            step_count+=1\n",
    "            \n",
    "            # check cost improvement\n",
    "            if cost_b >= cost_a:\n",
    "                print(\"Training loop : {}\".format(step_count))\n",
    "                print(\"Error : {}\".format(cost_b))\n",
    "                break\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return np.dot(X,self.weigths)+self.bias\n",
    "    \n",
    "    def cost_fn(self,Y,preds):\n",
    "        return np.mean(np.square(Y-preds))\n",
    "    \n",
    "    def gradient(self,X,Y,preds):\n",
    "        # gradient lse loss W -> x_i*-2(y_j−(w_0*x_0+w_1*x_1+..+w_n*x_n+b))\n",
    "        gradient_weigth = -2*(np.dot((Y-preds),X))/Y.shape[0]\n",
    "        \n",
    "        # gradient lse loss b -> (y_j*−2(w_0*x_0+w_1*x_1+..+w_n*x_n+b))\n",
    "        gradient_bias = -2*(np.mean(Y-preds))\n",
    "        \n",
    "        return gradient_weigth, gradient_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop : 235\n",
      "Error : 8.877643412130962e-30\n",
      "W : [3. 5.]\n",
      "b : 3.999999999999999\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(learning_rate = 0.1)\n",
    "model.fit(x,y)\n",
    "\n",
    "print(\"W : {}\".format(model.weigths))\n",
    "print(\"b : {}\".format(model.bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "### Given credit card fraud data below, fit a classification model on the trainData and evaluate the model on the testData using Area under curve of ROC (AUC) as a metric\n",
    "### The label column is \"Class\" and other columns are anonymized features\n",
    "### The data is sample from this dataset: https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"trainData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"testData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.1, max_iter = 100):\n",
    "        self.weigths = None\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        # init weigths parameter\n",
    "        self.weigths = np.zeros(X.shape[1]+1)\n",
    "        step_count = 0\n",
    "        \n",
    "        \n",
    "        # learn until cost not improve\n",
    "        for _ in range(self.max_iter):\n",
    "            pred_a = self.predict_prob(X)\n",
    "            cost_a = self.cost_fn(Y,pred_a)\n",
    "            \n",
    "            grad_w = self.gradient(X,Y,pred_a)\n",
    "            \n",
    "            # update weigth parameters\n",
    "            self.weigths = self.weigths - self.learning_rate*grad_w\n",
    "\n",
    "            pred_b = self.predict_prob(X)\n",
    "            cost_b = self.cost_fn(Y,pred_b)\n",
    "            \n",
    "            assert not np.isnan(cost_b)\n",
    "            step_count+=1\n",
    "            \n",
    "            # check cost improvement\n",
    "            if cost_b >= cost_a or (step_count+1)%100 == 0:\n",
    "                \n",
    "                print(\"Training loop : {}\".format(step_count))\n",
    "                print(\"Error : {}\".format(cost_b))\n",
    "                \n",
    "                if cost_b >= cost_a:\n",
    "                    break\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        bx = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X,bx), axis=1)\n",
    "        hx = np.dot(X, self.weigths)\n",
    "        return self.sigmoid(hx)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        bx = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X,bx), axis=1)\n",
    "        hx = np.dot(X,self.weigths)\n",
    "        hx = self.sigmoid(hx)\n",
    "        return hx.round()\n",
    "    \n",
    "    def cost_fn(self,Y,preds):\n",
    "        \n",
    "        term1 = Y*np.log(preds)\n",
    "        term2 = (1-Y)*np.log(1-preds)\n",
    "        cost = term1 + term2\n",
    "\n",
    "        return np.mean(-cost)\n",
    "    \n",
    "    def gradient(self,X,Y,preds):\n",
    "        bx = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X,bx), axis=1)\n",
    "        gradient_weigth = np.dot(X.T,  (preds - Y))/X.shape[0]\n",
    "        \n",
    "        return gradient_weigth\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        preds = self.predict(X)\n",
    "        return np.mean((Y==preds).all())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainData.drop('Class', axis=1).values\n",
    "Y_train = trainData['Class'].values\n",
    "X_test = testData.drop('Class', axis=1).values\n",
    "Y_test = testData['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop : 99\n",
      "Error : 0.2077993156346036\n",
      "Training loop : 199\n",
      "Error : 0.207798989296608\n",
      "Training loop : 299\n",
      "Error : 0.2077986631523494\n",
      "Training loop : 399\n",
      "Error : 0.20779833720165294\n",
      "Training loop : 499\n",
      "Error : 0.2077980114443439\n",
      "Training loop : 599\n",
      "Error : 0.20779768588024783\n",
      "Training loop : 699\n",
      "Error : 0.2077973605091903\n",
      "Training loop : 799\n",
      "Error : 0.20779703533099736\n",
      "Training loop : 899\n",
      "Error : 0.207796710345495\n",
      "Training loop : 999\n",
      "Error : 0.20779638555250957\n",
      "Training loop : 1099\n",
      "Error : 0.20779606095186753\n",
      "Training loop : 1199\n",
      "Error : 0.20779573654339564\n",
      "Training loop : 1299\n",
      "Error : 0.20779541232692073\n",
      "Training loop : 1399\n",
      "Error : 0.20779508830226992\n",
      "Training loop : 1499\n",
      "Error : 0.20779476446927053\n",
      "W : [-3.77499696e-05 -1.85447123e-07  1.60612317e-07 -4.05119250e-07\n",
      "  2.04321387e-07 -1.14297082e-07 -7.87352761e-08 -2.41633830e-07\n",
      "  2.11177340e-08 -1.36809931e-07 -2.71139451e-07  1.55111634e-07\n",
      " -2.77177560e-07 -1.87657475e-08 -3.68844999e-07 -1.65994400e-08\n",
      " -1.99494103e-07 -3.34044209e-07 -9.48860464e-08  3.82942834e-08\n",
      "  1.09663106e-08  4.39117329e-08  1.08107420e-08 -1.23912340e-09\n",
      " -5.31140886e-09 -1.11298897e-08 -4.79897402e-10  9.06165843e-09\n",
      "  4.51262937e-09 -2.57137621e-06 -5.74153480e-08]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(learning_rate = 1e-9, max_iter=1500)\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "print(\"W : {}\".format(model.weigths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9762308818226173"
      ]
     },
     "execution_count": 1387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "l = LogisticRegression(C = 1, penalty = 'l1')\n",
    "l.fit(X_train,Y_train)\n",
    "pred = l.predict(X_test)\n",
    "roc_auc_score(pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
