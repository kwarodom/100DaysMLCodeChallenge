{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics Essentials for Machine Learning Engineers\n",
    "\n",
    "A formal characterization of probability (conditional probability, Bayes rule, likelihood, independence, etc.) and techniques derived from it (Bayes Nets, Markov Decision Processes, Hidden Markov Models, etc.) are at the heart of many Machine Learning algorithms; these are a means to deal with uncertainty in the real world. Closely related to this is the field of statistics, which provides various measures (mean, median, variance, etc.), distributions (uniform, normal, binomial, Poisson, etc.) and analysis methods (ANOVA, hypothesis testing, etc.) that are necessary for building and validating models from observed data. Many Machine Learning algorithms are essentially extensions of statistical modeling procedures.\n",
    "\n",
    "## Probability and  Statistic\n",
    "- Basic Concepts   \n",
    "- Probability Space  \n",
    "- Random Variable\n",
    "- Distribution, Joint Distribution and Marginal Distribution\n",
    "- Conditional Distributions\n",
    "- Independence\n",
    "- Chain Rule and Bayes Rule\n",
    "    - Naive Bayes\n",
    "    - Markov Chain\n",
    "\n",
    "**Probability Distribution**\n",
    "- Discrete Distribution\n",
    "- Probability Mass Function\n",
    "- Continuous Distribution\n",
    "- Probability Density Function\n",
    "\t\n",
    "**Expectations and Variance**\n",
    "\n",
    "**Important Distributions**\n",
    "- Bernoulli\n",
    "- Poisson\n",
    "- Gaussian\n",
    "\n",
    "## Linear Algebra\n",
    "**Operation on Vectors and Matrices**\n",
    "- Vector Arithmetic\n",
    "    - Addition of two vectors\n",
    "    - Scalar Multiplication\n",
    "    - Dot Product\n",
    "    - Vector Projection\n",
    "\n",
    "- Matrix Arithmetic\n",
    "    - Addition of two Matrices\n",
    "    - Scalar Multiplication\n",
    "    - Matrix-Matrix Multiplication\n",
    "\n",
    "**Optimization Theory**\n",
    "- Maximum Likelihood\n",
    "- Expectation Maximization\n",
    "- Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is **10 free machine learning books**\n",
    "1. [Probability and Statistics for Programmers](http://www.greenteapress.com/thinkstats/)\n",
    "2. [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/091117.pdf)\n",
    "2. [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "2. [Understanding Machine Learning](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)\n",
    "2. [A Programmerâ€™s Guide to Data Mining](http://guidetodatamining.com/)\n",
    "2. [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf)\n",
    "2. [A Brief Introduction to Neural Networks](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf)\n",
    "2. [Deep Learning](http://www.deeplearningbook.org/)\n",
    "2. [Natural Language Processing with Python](https://www.researchgate.net/publication/220691633_Natural_Language_Processing_with_Python)\n",
    "2. [Machine Learning Yearning](http://www.mlyearning.org/)\n",
    "<a id=\"5\"></a> <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
